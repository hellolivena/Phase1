{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406ddb48-0bdf-42fa-9577-0387afcdba2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #%%\n",
    "# ######################## Connect Python ##################\n",
    "# import snowflake.connector as snow\n",
    "# import pandas as pd\n",
    "# user= 'ayush.pandey1-cw@otsuka-us.com'\n",
    "\n",
    "# conn = snow.connect(\n",
    "#             host='otsuka_ctprod.us-east-1.snowflakecomputing.com',\n",
    "#             database='cdr',\n",
    "#             user= user,\n",
    "#             authenticator='externalbrowser',account='otsuka_ctprod.us-east-1',port=443)\n",
    "\n",
    "\n",
    "# cursor = conn.cursor()\n",
    "\n",
    "# cursor.execute('USE DATABASE CDR')\n",
    "# cursor.execute('USE SCHEMA operational_analytics')\n",
    "# cursor.execute('USE WAREHOUSE OPA_MIG_M_WH')\n",
    "\n",
    "# query = \"SELECT REGION_NAME,FRIDAY_END,SALES,CALLS FROM CT_TREND_MDD_FIELD_SALE_CALL where region_name=geo_name\"\n",
    "# # query = \"SELECT * FROM RSC_DATES\"\n",
    "\n",
    "\n",
    "# # Execute the query and load results into a Pandas DataFrame\n",
    "# try:\n",
    "#     weekly_sales_data= pd.read_sql(query, conn)\n",
    "#     # Display the first 5 rows of the DataFrame\n",
    "# finally:\n",
    "#     conn.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57fe167-8f7c-4b90-a0f8-14c8ddfc1da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "weekly_sales_data=pd.read_csv('all_team_sale_call.csv')\n",
    "ct=datetime.now()\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b422dd-68f3-45e8-8a3b-13ffa93192a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from io import StringIO\n",
    "import itertools\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "import threading\n",
    "from datetime import datetime\n",
    "import random\n",
    "#%%\n",
    "week_data_dist=weekly_sales_data\n",
    "week_data=weekly_sales_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d458fccc-67b3-4f8d-a588-9f28709332fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "week_data['FRIDAY_END'] = pd.to_datetime(week_data['FRIDAY_END'], format='%Y-%m-%d')\n",
    "week_data = week_data[week_data[\"REGION_NAME\"] == week_data[\"GEO_NAME\"]]\n",
    "# Sort by date to ensure chronological order\n",
    "week_data = week_data.sort_values(by=['TEAM','REGION_NAME','GEO_NAME','FRIDAY_END'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebce8549-9c31-4f15-a973-94f99cdf8929",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "def best_value_find(team,region_name,week_data):\n",
    "    global para_df\n",
    "    print(\"enter Region:\",team,region,datetime.now())\n",
    "    team_name=team\n",
    "    week_data_region=week_data\n",
    "    reg_name=region_name\n",
    "    # Check the size of the dataset\n",
    "    print(f\"Dataset size: {len(week_data_region)}\")\n",
    "    if week_data_region.empty:\n",
    "        raise ValueError(\"The dataset is empty for the given region and task.\")\n",
    "\n",
    "    # Split into test (latest 10 weeks) and train (remaining data)\n",
    "    test_df = week_data_region.tail(6)  # Latest 10 weeks\n",
    "    train_df = week_data_region.iloc[:-6]  # Rest of the data\n",
    "    \n",
    "    train_data = train_df['SALES']\n",
    "    train_data_calls=train_df['CALLS']\n",
    "    full_data=week_data_region['SALES']\n",
    "    full_data_calls=week_data_region['CALLS']\n",
    "    test_data = test_df['SALES']\n",
    "    test_data_calls=test_df['CALLS']\n",
    "    \n",
    "    \n",
    "    # Ignore all warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "    # Define parameter ranges\n",
    "    p = range(1, 2)   # p: 0-4\n",
    "    d = range(0, 1)   # d: 0-1\n",
    "    q = range(1, 2)   # q: 0-4\n",
    "    P = range(0, 1)   # P: 0-3\n",
    "    D = range(1, 2)   # D: 0-1\n",
    "    Q = range(0, 2)   # Q: 0-3\n",
    "    m = 52            # Seasonal period (monthly data)\n",
    "    \n",
    "    # Create a list to store results\n",
    "    results = []\n",
    "    i = 1\n",
    "    print(\"b\")\n",
    "    # Loop through all parameter combinations\n",
    "    for (p_val, d_val, q_val, P_val, D_val, Q_val) in itertools.product(p, d, q, P, D, Q):\n",
    "        # print(\"R:\",reg_name,\"C:\",i)\n",
    "        print(\"R:\",reg_name,\"T:\",team,\"C:\",i,)\n",
    "        i += 1\n",
    "        try:\n",
    "            # Fit SARIMA model\n",
    "            model = SARIMAX(\n",
    "                train_data_calls,\n",
    "                order=(p_val, d_val, q_val),\n",
    "                seasonal_order=(P_val, D_val, Q_val, m),\n",
    "                enforce_stationarity=False,\n",
    "                enforce_invertibility=False,\n",
    "                # exog=train_data_calls\n",
    "            )\n",
    "            fitted_model = model.fit(disp=False)\n",
    "            forecast_values = fitted_model.forecast(steps=len(test_data_calls))\n",
    "            # Adjust the index of the forecast values to match the test data\n",
    "            forecast_values.index = test_data.index\n",
    "            \n",
    "            # Calculate evaluation metrics\n",
    "            mae = mean_absolute_error(test_data, forecast_values)\n",
    "            rmse = np.sqrt(mean_squared_error(test_data, forecast_values))\n",
    "            \n",
    "            # Max-Min Differences\n",
    "            max_min_actual = test_data.max() - test_data.min()\n",
    "            max_min_fitted = forecast_values.max() - forecast_values.min()\n",
    "        \n",
    "            # Calculate the count of same slope trends (i.e., compare differences between successive data points)\n",
    "            actual_diff = np.sign(np.diff(test_data))  # Sign of the difference between successive points\n",
    "            fitted_diff = np.sign(np.diff(forecast_values))  # Same for fitted values\n",
    "            same_slope_count = np.sum(actual_diff == fitted_diff)\n",
    "            \n",
    "            # Store results\n",
    "            results.append({\n",
    "                'order': (p_val, d_val, q_val),\n",
    "                'seasonal_order': (P_val, D_val, Q_val, m),\n",
    "                'AIC': fitted_model.aic,\n",
    "                'MAE': mae,\n",
    "                'RMSE': rmse,\n",
    "                'Max-Min Actual': max_min_actual,\n",
    "                'Max-Min Fitted': max_min_fitted,\n",
    "                'Count Same Slope Trend': same_slope_count\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Handle any errors (e.g., convergence issues)\n",
    "            print(f\"Error for parameters: p={p_val}, d={d_val}, q={q_val}, P={P_val}, D={D_val}, Q={Q_val}, m={m}\")\n",
    "            print(str(e))\n",
    "    \n",
    "    # Convert results to DataFrame for better visualization\n",
    "    results_df= pd.DataFrame(results)\n",
    "    # print(3)\n",
    "    # print(\"Loop1:\",team,reg_name)\n",
    "    df=results_df[['order','seasonal_order','AIC','MAE','RMSE','Max-Min Fitted','Count Same Slope Trend']]\n",
    "    # df\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    max_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    # print(4)\n",
    "    \n",
    "    # Normalize columns to be minimized (AIC, MAE, RMSE, Max-Min Fitted)\n",
    "    df[['aic2_scaled', 'mse_scaled', 'rmse2_scaled', 'fitted_diff_scaled']] = min_max_scaler.fit_transform(df[['AIC', 'MAE', 'RMSE', 'Max-Min Fitted']])\n",
    "    \n",
    "    # Normalize the column to be maximized (Count Same Slope Trend) - we use max scaler to maximize it\n",
    "    df['count_scaled'] = max_scaler.fit_transform(df[['Count Same Slope Trend']])\n",
    "    # print(5)\n",
    "    \n",
    "    # Calculate a combined score. Minimized columns contribute negatively, maximized column contributes positively.\n",
    "    df['score'] = (\n",
    "    0.05 * df['aic2_scaled'] + 0.3* df['mse_scaled'] + 0.45 * df['rmse2_scaled'] + 0.2 * df['fitted_diff_scaled']-0.3*df['count_scaled'])\n",
    "    # df['score'] = (df['aic2_scaled'] + df['mse_scaled'] + df['rmse2_scaled'] + df['fitted_diff_scaled']) - df['count_scaled']\n",
    "    \n",
    "    # Sort the DataFrame by the score in ascending order (lowest score is best)\n",
    "    df_sorted = df.sort_values(by='score', ascending=True)\n",
    "    # print(6)\n",
    "    \n",
    "    # Select the top 5 most optimized rows (lowest scores)\n",
    "    best_value = df_sorted.head(1)\n",
    "    \n",
    "    # Output the top 5 most optimized rows\n",
    "    # best_value\n",
    "    # print(7)\n",
    "    # Convert the single row to a dictionary\n",
    "    row_dict = best_value.iloc[0].to_dict()\n",
    "    \n",
    "    # Extract p, d, q from 'order' and P, D, Q from 'seasonal_order'\n",
    "    order = row_dict['order']\n",
    "    seasonal_order = row_dict['seasonal_order']\n",
    "    \n",
    "    # Extract values from the tuples\n",
    "    p1, d1, q1 = order\n",
    "    P1, D1, Q1, m = seasonal_order  # m is not needed for extraction\n",
    "    \n",
    "    \n",
    "    model = SARIMAX(\n",
    "            full_data_calls,\n",
    "            order=(p1, d1, q1),\n",
    "            seasonal_order=(P1, D1, Q1, m),\n",
    "            enforce_stationarity=False,\n",
    "            enforce_invertibility=False,\n",
    "            # exog=train_data_calls\n",
    "        )\n",
    "    fitted_model = model.fit(disp=False)\n",
    "    Forecasted_calls = fitted_model.forecast(steps=12)\n",
    "    # print(8)\n",
    "    \n",
    "    # Assuming `forecast` is the array of predicted values\n",
    "    mean_data = (full_data_calls.mean())\n",
    "    max_data = (full_data_calls.max())\n",
    "    max_value=(max_data/mean_data)*1.1*mean_data\n",
    "    min_value= (mean_data/max_data)*0.8*mean_data\n",
    "    \n",
    "    # Clip the forecasted values\n",
    "    forecasted_calls = np.clip(Forecasted_calls, min_value, max_value)\n",
    "    \n",
    "    # Ignore all warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "    # Define parameter ranges\n",
    "    p = range(1, 2)   # p: 0-4\n",
    "    d = range(0, 1)   # d: 0-1\n",
    "    q = range(1, 2)   # q: 0-4\n",
    "    P = range(0, 1)   # P: 0-3\n",
    "    D = range(1, 2)   # D: 0-1\n",
    "    Q = range(0, 2)   # Q: 0-3\n",
    "    m = 52            # Seasonal period (monthly data)\n",
    "    \n",
    "    # Create a list to store results\n",
    "    results = []\n",
    "    i = 1\n",
    "    # print(9)\n",
    "    \n",
    "    # Loop through all parameter combinations\n",
    "    for (p_val, d_val, q_val, P_val, D_val, Q_val) in itertools.product(p, d, q, P, D, Q):\n",
    "        print(\"R:\",reg_name,\"SC:\",i)\n",
    "        i += 1\n",
    "        \n",
    "        try:\n",
    "            # Fit SARIMA model\n",
    "            model = SARIMAX(\n",
    "                train_data,\n",
    "                order=(p_val, d_val, q_val),\n",
    "                seasonal_order=(P_val, D_val, Q_val, m),\n",
    "                enforce_stationarity=False,\n",
    "                enforce_invertibility=False,\n",
    "                exog=train_data_calls\n",
    "            )\n",
    "            fitted_model = model.fit(disp=False)\n",
    "            forecast_values = fitted_model.forecast(steps=len(test_data), exog=test_data_calls)\n",
    "            # Adjust the index of the forecast values to match the test data\n",
    "            forecast_values.index = test_data.index\n",
    "            \n",
    "            # Calculate evaluation metrics\n",
    "            mae = mean_absolute_error(test_data, forecast_values)\n",
    "            rmse = np.sqrt(mean_squared_error(test_data, forecast_values))\n",
    "            \n",
    "            # Max-Min Differences\n",
    "            max_min_actual = test_data.max() - test_data.min()\n",
    "            max_min_fitted = forecast_values.max() - forecast_values.min()\n",
    "        \n",
    "            # Calculate the count of same slope trends (i.e., compare differences between successive data points)\n",
    "            actual_diff = np.sign(np.diff(test_data))  # Sign of the difference between successive points\n",
    "            fitted_diff = np.sign(np.diff(forecast_values))  # Same for fitted values\n",
    "            same_slope_count = np.sum(actual_diff == fitted_diff)\n",
    "            \n",
    "            # Store results\n",
    "            results.append({\n",
    "                'order': (p_val, d_val, q_val),\n",
    "                'seasonal_order': (P_val, D_val, Q_val, m),\n",
    "                'AIC': fitted_model.aic,\n",
    "                'MAE': mae,\n",
    "                'RMSE': rmse,\n",
    "                'Max-Min Actual': max_min_actual,\n",
    "                'Max-Min Fitted': max_min_fitted,\n",
    "                'Count Same Slope Trend': same_slope_count\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Handle any errors (e.g., convergence issues)\n",
    "            print(f\"Error for parameters: p={p_val}, d={d_val}, q={q_val}, P={P_val}, D={D_val}, Q={Q_val}, m={m}\")\n",
    "            print(str(e))\n",
    "    # print(10)\n",
    "    \n",
    "    # Convert results to DataFrame for better visualization\n",
    "    results_df_2 = pd.DataFrame(results)\n",
    "    df=results_df_2[['order','seasonal_order','AIC','MAE','RMSE','Max-Min Fitted','Count Same Slope Trend']]\n",
    "    # df\n",
    "        \n",
    "    # Initialize scalers\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    max_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    \n",
    "    # Normalize columns to be minimized (AIC, MAE, RMSE, Max-Min Fitted)\n",
    "    df[['aic2_scaled', 'mse_scaled', 'rmse2_scaled', 'fitted_diff_scaled']] = min_max_scaler.fit_transform(df[['AIC', 'MAE', 'RMSE', 'Max-Min Fitted']])\n",
    "    \n",
    "    # Normalize the column to be maximized (Count Same Slope Trend) - we use max scaler to maximize it\n",
    "    df['count_scaled'] = max_scaler.fit_transform(df[['Count Same Slope Trend']])\n",
    "    \n",
    "    # Calculate a combined score. Minimized columns contribute negatively, maximized column contributes positively.\n",
    "    df['score'] = (\n",
    "    0.05 * df['aic2_scaled'] + 0.3* df['mse_scaled'] + 0.45 * df['rmse2_scaled'] + 0.2 * df['fitted_diff_scaled']-0.3*df['count_scaled'])\n",
    "    # df['score'] = (df['aic2_scaled'] + df['mse_scaled'] + df['rmse2_scaled'] + df['fitted_diff_scaled']) - df['count_scaled']\n",
    "    \n",
    "    # Sort the DataFrame by the score in ascending order (lowest score is best)\n",
    "    df_sorted = df.sort_values(by='score', ascending=True)\n",
    "    \n",
    "    # Select the top 5 most optimized rows (lowest scores)\n",
    "    best_value_2 = df_sorted.head(1)\n",
    "    # print(11)\n",
    "    # Convert the single row to a dictionary\n",
    "    row_dict = best_value_2.iloc[0].to_dict()\n",
    "    # print(\"Loop2:\",team,reg_name)\n",
    "    # Extract p, d, q from 'order' and P, D, Q from 'seasonal_order'\n",
    "    order = row_dict['order']\n",
    "    seasonal_order = row_dict['seasonal_order']\n",
    "    # print(12)\n",
    "    # Extract values from the tuples\n",
    "    p2, d2, q2 = order\n",
    "    P2, D2, Q2, m = seasonal_order  # m is not needed for extraction\n",
    "    \n",
    "    para_reg=[team_name,reg_name,p1,d1,q1,P1,D1,Q1,p2,d2,q2,P2,D2,Q2]\n",
    "    para_reg_df = pd.DataFrame([para_reg], columns=['team','reg_name','p1','d1','q1','P1','D1','Q1','p2','d2','q2','P2','D2','Q2'])\n",
    "    print(\"exit Region:\",team,region,datetime.now())\n",
    "    return para_reg_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99a2bd0-a816-4845-af17-476a8f432006",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#%%\n",
    "para_col=['team','reg_name','p1','d1','q1','P1','D1','Q1','p2','d2','q2','P2','D2','Q2']\n",
    "para_df=pd.DataFrame(columns=para_col)\n",
    "\n",
    "# Lock for thread safety\n",
    "lock = threading.Lock()\n",
    "\n",
    "# Thread target function\n",
    "def threaded_best_value(team,region, region_data):\n",
    "    global para_df\n",
    "    # Compute best value\n",
    "    result = best_value_find(team,region, region_data)\n",
    "    \n",
    "    # Safely append to para_df\n",
    "    with lock:\n",
    "        para_df = pd.concat([para_df, result], ignore_index=True)\n",
    "\n",
    "# List to store threads\n",
    "threads = []\n",
    "\n",
    "# Start threads for each region\n",
    "for team, region in week_data_dist[['TEAM', 'REGION_NAME']].drop_duplicates().values:\n",
    "        region_data = week_data[(week_data['REGION_NAME'] == region) & (week_data['TEAM'] == team)]\n",
    "        # print (region_data.head(1))\n",
    "        thread = threading.Thread(target=threaded_best_value, args=(team,region, region_data))\n",
    "        thread.start()\n",
    "        threads.append(thread)\n",
    "\n",
    "# Wait for all threads to complete\n",
    "for thread in threads:\n",
    "    thread.join()\n",
    "\n",
    "# Final output\n",
    "print(\"Final DataFrame:\")\n",
    "print(para_df)\n",
    "\n",
    "final_df=para_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89655913-d86c-4f19-9e7e-f02c32e6e1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "final_df.to_csv('best_arima_para.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c557e6-2729-4d23-8edc-fda88c05a859",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "week_data_dist['FRIDAY_END'] = pd.to_datetime(week_data_dist['FRIDAY_END'], format='%Y-%m-%d')\n",
    "\n",
    "#%%\n",
    "def forecast_region_data(team_name,geo_name,reg,dist_data,p1,d1,q1,P1,D1,Q1,p2,d2,q2,P2,D2,Q2):\n",
    "    team=team_name\n",
    "    region=reg\n",
    "    geo=geo_name\n",
    "    geo_data=dist_data\n",
    "    geo_data_sales=geo_data['SALES']\n",
    "    geo_data_calls=geo_data['CALLS']\n",
    "    m=52\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    model = SARIMAX(\n",
    "                geo_data_calls,\n",
    "                order=(p1, d1, q1),\n",
    "                seasonal_order=(P1, D1, Q1, m),\n",
    "                enforce_stationarity=False,\n",
    "                enforce_invertibility=False,\n",
    "                # exog=full_data_calls\n",
    "            )\n",
    "    fitted_model = model.fit(disp=False)\n",
    "    Forecasted_calls = fitted_model.forecast(steps=12)\n",
    "    # print(Forecasted_calls)\n",
    "    mean_data = (geo_data_calls.mean())\n",
    "    max_data = (geo_data_calls.max())\n",
    "    max_value=(max_data/mean_data)*1.1*mean_data\n",
    "    min_value= (mean_data/max_data)*0.8*mean_data\n",
    "    # Clip the forecasted values\n",
    "    forecasted_calls = np.clip(Forecasted_calls, min_value, max_value)\n",
    "    # print(forecasted_calls)\n",
    "    \n",
    "    model = SARIMAX(\n",
    "                geo_data_sales,\n",
    "                order=(p2, d2, q2),\n",
    "                seasonal_order=(P2, D2, Q2, m),\n",
    "                enforce_stationarity=False,\n",
    "                enforce_invertibility=False,\n",
    "                exog=geo_data_calls\n",
    "            )\n",
    "    fitted_model = model.fit(disp=False)\n",
    "    Forecasted_sales = fitted_model.forecast(steps=12,exog=forecasted_calls)\n",
    "    mean_data = (geo_data_sales.mean())\n",
    "    max_data = (geo_data_sales.max())\n",
    "    max_value=(max_data/mean_data)*1.1*mean_data\n",
    "    min_value= (mean_data/max_data)*0.8*mean_data\n",
    "\n",
    "    numbers = [round(x, 2) for x in [1.0 + i * 0.01 for i in range(1, 11)]]\n",
    "    # Clip the forecasted values\n",
    "    forecasted_sales_1 = np.clip(Forecasted_sales, min_value, max_value)\n",
    "    forecasted_sales = []\n",
    "    for value in forecasted_sales_1:\n",
    "        if value==max_value:\n",
    "            random_number = random.choice(numbers)\n",
    "            forecasted_sales.append(value * random_number)\n",
    "        else:\n",
    "            forecasted_sales.append(value)\n",
    "    # print(forecasted_sales)\n",
    "    \n",
    "    last_date = geo_data['FRIDAY_END'].max()\n",
    "    forecast_dates = [last_date + pd.Timedelta(weeks=i) for i in range(1, 13)]\n",
    "    forecast_data = pd.DataFrame({\n",
    "        \"FRIDAY_END\": forecast_dates,\n",
    "        \"SALES\": forecasted_sales,\n",
    "        \"CALLS\": forecasted_calls,\n",
    "        \"SALE_TYPE\": \"Forecast\"  # Set sale type to \"Forecast\"\n",
    "    })\n",
    "    # print(forecast_data)\n",
    "    geo_data['SALE_TYPE'] = \"Actual\"\n",
    "    forecast_data['REGION_NAME'] = region\n",
    "    forecast_data['GEO_NAME'] = geo\n",
    "    forecast_data['TEAM'] = team\n",
    "    \n",
    "    \n",
    "    combined_data = pd.concat([geo_data, forecast_data], ignore_index=True)\n",
    "    return combined_data\n",
    "#%%\n",
    "\n",
    "col_data = [\"TEAM\", \"REGION_NAME\", \"GEO_NAME\", \"FRIDAY_END\", \"SALES\", \"CALLS\", \"SALE_TYPE\"]\n",
    "final_forecast = pd.DataFrame(columns=col_data)\n",
    "\n",
    "comb_data_list = []\n",
    "for team, region, geo in week_data_dist[['TEAM', 'REGION_NAME', 'GEO_NAME']].drop_duplicates().values:\n",
    "    print(team, region, geo)\n",
    "    geo_data = week_data_dist[(week_data_dist['GEO_NAME'] == geo) & \n",
    "                              (week_data_dist['TEAM'] == team) & \n",
    "                              (week_data_dist['REGION_NAME'] == region)]\n",
    "    \n",
    "    region_best_value = final_df[(final_df['reg_name'] == region) & (final_df['team'] == team)]\n",
    "    if region_best_value.empty:\n",
    "        print(f\"No matching data for team: {team}, region: {region}\")\n",
    "        continue\n",
    "\n",
    "    # Ensure numeric casting\n",
    "    region_best_value = region_best_value.astype({\n",
    "        'p1': 'int', 'd1': 'int', 'q1': 'int',\n",
    "        'P1': 'int', 'D1': 'int', 'Q1': 'int',\n",
    "        'p2': 'int', 'd2': 'int', 'q2': 'int',\n",
    "        'P2': 'int', 'D2': 'int', 'Q2': 'int'\n",
    "    })\n",
    "\n",
    "    p1 = region_best_value['p1'].iloc[0]\n",
    "    d1 = region_best_value['d1'].iloc[0]\n",
    "    q1 = region_best_value['q1'].iloc[0]\n",
    "    P1 = region_best_value['P1'].iloc[0]\n",
    "    D1 = region_best_value['D1'].iloc[0]\n",
    "    Q1 = region_best_value['Q1'].iloc[0]\n",
    "    p2 = region_best_value['p2'].iloc[0]\n",
    "    d2 = region_best_value['d2'].iloc[0]\n",
    "    q2 = region_best_value['q2'].iloc[0]\n",
    "    P2 = region_best_value['P2'].iloc[0]\n",
    "    D2 = region_best_value['D2'].iloc[0]\n",
    "    Q2 = region_best_value['Q2'].iloc[0]\n",
    "\n",
    "    comb_data = forecast_region_data(team, region, geo, geo_data, p1, d1, q1, P1, D1, Q1, p2, d2, q2, P2, D2, Q2)\n",
    "    if not isinstance(comb_data, pd.DataFrame):\n",
    "        raise ValueError(\"forecast_region_data must return a DataFrame\")\n",
    "    comb_data_list.append(comb_data)\n",
    "\n",
    "final_forecast = pd.concat(comb_data_list, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc646162-c3a3-499f-9ad6-ffbf814b28bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "ct=datetime.now()\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dafcc8-5d47-4f12-9830-3d14b32e9b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #%%\n",
    "# final_forecast.to_csv('final_forecast.csv',index=False,header=False)\n",
    "\n",
    "# #%%\n",
    "# user= 'ayush.pandey1-cw@otsuka-us.com'\n",
    "\n",
    "# conn = snow.connect(\n",
    "#             host='otsuka_ctprod.us-east-1.snowflakecomputing.com',\n",
    "#             database='cdr',\n",
    "#             user= user,\n",
    "#             authenticator='externalbrowser',account='otsuka_ctprod.us-east-1',port=443)\n",
    "\n",
    "\n",
    "# cursor = conn.cursor()\n",
    "# #%%\n",
    "# cursor.execute('USE DATABASE CDR')\n",
    "# cursor.execute('USE SCHEMA operational_analytics')\n",
    "# cursor.execute('USE WAREHOUSE OPA_MIG_M_WH')\n",
    "# cursor.execute(\"\"\"CREATE OR REPLACE TABLE CT_TREND_FIELD_SALES_CALL (\n",
    "#     TEAM STRING,\n",
    "#     REGION_NAME STRING,\n",
    "#     GEO_NAME STR\n",
    "#     FRIDAY_END STRING,\n",
    "#     SALES STRING,\n",
    "#     CALLS STRING,\n",
    "#     SALE_TYPE STRING\n",
    "# );\"\"\")\n",
    "# #%%\n",
    "# # Create a Snowflake internal stage (if it doesn't exist)\n",
    "# cursor.execute(\"CREATE OR REPLACE STAGE my_stage\")\n",
    "\n",
    "# # Upload CSV file to Snowflake stage\n",
    "# cursor.execute(\"PUT file://final_forecast.csv @my_stage\")\n",
    "# #%%\n",
    "# # Step 3: Copy data from the stage into the Snowflake table\n",
    "# cursor.execute(\"\"\"\n",
    "#     COPY INTO CT_TREND_FIELD_SALES_CALL\n",
    "#     FROM @my_stage/final_forecast.csv\n",
    "#     FILE_FORMAT = (TYPE = 'CSV' FIELD_OPTIONALLY_ENCLOSED_BY='\"')\n",
    "# \"\"\")\n",
    "\n",
    "# print(\"Data loaded into Snowflake successfully.\")\n",
    "\n",
    "# cursor.close()\n",
    "# conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13bc759-0d72-4668-aef1-b8c4e9a94e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "final_forecast.to_csv('final_forecast.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cded3f6-01c3-42b3-b689-f3a5a6bc5be5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
